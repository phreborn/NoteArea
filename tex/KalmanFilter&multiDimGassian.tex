\section{多维高斯函数}

高斯分布函数：
\begin{equation}
p(x) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}
\end{equation}

\subsection{两个高斯分布相乘}
\begin{align}
    &\frac{1}{\sqrt{2\pi}\sigma_{1}}e^{-\frac{(x-\mu_{1})^2}{2\sigma_{1}^2}}
    \frac{1}{\sqrt{2\pi}\sigma_{2}}e^{-\frac{(x-\mu_{2})^2}{2\sigma_{2}^2}}=?\\
    \\
    &\frac{(x-\mu_{1})^2}{2\sigma_{1}^2} + \frac{(x-\mu_{2})^2}{2\sigma_{2}^2}\\
    =& \frac{(\sigma_{1}^2+\sigma_{2}^2)x^2+\sigma_{2}^2\mu_{1}^2+\sigma_{1}^2\mu_{2}^2-2(\sigma_{2}^2\mu_{1}+\sigma_{1}^2\mu_{2})x}{2(\sigma_{1}\sigma_{2})^2}\\
    =& \frac{(\sigma_{1}^2+\sigma_{2}^2)(x-\frac{\sigma_{2}^2\mu_{1}+\sigma_{1}^2\mu_{2}}{\sigma_{1}^2+\sigma_{2}^2})^2- \frac{(\sigma_{2}^2\mu_{1}+\sigma_{1}^2\mu_{2})^2}{\sigma_{1}^2+\sigma_{2}^2} +\sigma_{2}^2\mu_{1}^2+\sigma_{1}^2\mu_{2}^2}{2(\sigma_{1}\sigma_{2})^2}\\
    \\
    &\frac{1}{\sqrt{2\pi}\sigma_{1}}e^{-\frac{(x-\mu_{1})^2}{2\sigma_{1}^2}}
    \frac{1}{\sqrt{2\pi}\sigma_{2}}e^{-\frac{(x-\mu_{2})^2}{2\sigma_{2}^2}}\\
    =& \frac{1}{2\pi\sigma_{1}\sigma_{2}}e^{-(\frac{(x-\mu_{1})^2}{2\sigma_{1}^2} + \frac{(x-\mu_{2})^2}{2\sigma_{2}^2})}\\
    \propto & Ce^{-\frac{(x-\frac{\sigma_{2}^2\mu_{1}+\sigma_{1}^2\mu_{2}}{\sigma_{1}^2+\sigma_{2}^2})^2}{\frac{2(\sigma_{1}\sigma_{2})^2}{\sigma_{1}^2+\sigma_{2}^2}}}\\
    =& Ce^{-\frac{(x-\mu')^2}{2\sigma'^{2}}}\\
    &\mu'=\frac{\sigma_{2}^2\mu_{1}+\sigma_{1}^2\mu_{2}}{\sigma_{1}^2+\sigma_{2}^2}\\
    &\sigma'=\frac{\sigma_{1}\sigma_{2}}{\sqrt{\sigma_{1}^2+\sigma_{2}^2}}
\end{align}

\subsection{多维高斯}
\begin{align}
    \label{eq:multiDimGassian1}
    &\frac{1}{\sqrt{2\pi}\sigma_{1}}e^{-\frac{(x_{1}-\mu_{1})^2}{2\sigma_{1}^2}}
    \frac{1}{\sqrt{2\pi}\sigma_{2}}e^{-\frac{(x_{2}-\mu_{2})^2}{2\sigma_{2}^2}}
    \dots
    \frac{1}{\sqrt{2\pi}\sigma_{n}}e^{-\frac{(x_{n}-\mu_{n})^2}{2\sigma_{n}^2}}\\
    =&\frac{1}{(2\pi)^{\frac{n}{2}}\prod_{i=1}^{n}\sigma_{i}}e^{-\sum_{i=1}^{n}\frac{(x_{i}-\mu_{i})^2}{2\sigma_{i}^2}}
\end{align}
\begin{align}
    X =&
    \begin{pmatrix}
        x_{1}\\
        x_{2}\\
        \vdots\\
        x_{n}
    \end{pmatrix}\\
    M =&
    \begin{pmatrix}
        \mu_{1}\\
        \mu_{2}\\
        \vdots\\
        \mu_{n}
    \end{pmatrix}\\
    \Sigma^{-1} =&
    \begin{pmatrix}
        \frac{1}{\sigma_{1}^2} & 0 & \dots & 0\\
        0 & \frac{1}{\sigma_{2}^2} & \dots & 0\\
        \vdots & \vdots & \ddots & \vdots\\
        0 & 0 & \dots & \frac{1}{\sigma_{n}^2}
    \end{pmatrix}
\end{align}
式\ref{eq:multiDimGassian1}等于：
\begin{align}
    \label{eq:multiDimGassian2}
    \frac{1}{(2\pi)^{\frac{n}{2}}(det(\Sigma))^{\frac{1}{2}}}e^{-\frac{1}{2}(X-M)^T\Sigma^{-1}(X-M)}
\end{align}
\\
\begin{align}
    \text{数据集包含m个n维数据}\ 
    \bar{X}=\begin{pmatrix}
        X^{1} & X^{2} & \dots & X^{m}
    \end{pmatrix}
\end{align}
\begin{align}
    \sigma_{ij} =& \frac{1}{m}\sum_{k=1}^{m}(x_{i}^{k}-\mu_{i})(x_{j}^{k}-\mu_{j})\\
    \mu_{i}=&E(x_{i})= \frac{1}{m}\sum_{k=1}^{m}x_{i}^{k}
\end{align}
\begin{gather}
    \Sigma = \begin{pmatrix}
        \sigma_{ij}
    \end{pmatrix}\\
    \text{由于}\sigma_{ij}=\sigma_{ji}\text{, 所以, }\Sigma^{T}=\Sigma
\end{gather}

\subsection{一点线性代数}
\begin{gather}
    A=\begin{pmatrix}
        a_{ij}
    \end{pmatrix}\\
    A^{\dagger}=(A^{T})^{*}\\
    A^{\dagger}_{ij} = (a_{ji})^{*}
\end{gather}
\begin{gather}
    A^{\dagger}=A\ (\text{即}(a_{ji})^{*}=a_{ij})\\
    \text{若存在列向量}X\text{使得}AX=\lambda X,\ \lambda\text{是实数}\\
    X^{\dagger}A=X^{\dagger}A^{\dagger}=(AX)^{\dagger}=(\lambda X)^{\dagger}=\lambda X^{\dagger}\\
    AX_{1}=\lambda_{1} X_{1}\\
    AX_{2}=\lambda_{2} X_{2}\\
    \lambda_{2}X_{2}^{\dagger}X_{1}=(X_{2}^{\dagger}A)X_{1}=X_{2}^{\dagger}AX_{1}=X_{2}^{\dagger}(AX_{1})=\lambda_{1}X_{2}^{\dagger}X_{1}\\
    \rightarrow \lambda_{2}=\lambda_{1}\ or\ X_{2}^{\dagger}X_{1}=0 \label{eq:OrthogonalEigenVector}
\end{gather}

\begin{gather}
    \label{eq:eigenvalue1}
    AX=\lambda X \rightarrow (A-\lambda I)X=0\\
    I=\begin{pmatrix}
        1 & 0 & \dots & 0\\
        0 & 1 & \dots & 0\\
        \vdots & \vdots & \ddots & \vdots\\
        0 & 0 & \dots & 1
    \end{pmatrix}
\end{gather}
式\ref{eq:eigenvalue1}有解，说明矩阵$(A-\lambda I)$的行(列)向量之间不是线性独立的，即：
\begin{gather}
    \label{eq:eigenvalue2}
    det(A-\lambda I)=0
\end{gather}
式\ref{eq:eigenvalue2}是关于$\lambda$的$N=Dim(A)$元方程，可解得$\{\lambda_{1}\ \lambda_{2}\ \dots\ \lambda_{N}\}$，称为矩阵$A$的本征值。
$A$的$N$个本征值对应$N$个本征向量$\{X_{1}\ X_{2}\ \dots\ X_{N}\}$
（如果求得的是$\{n_{1}\times\lambda_{1}\ n_{2}\times\lambda_{2}\ \dots\ n_{k}\times\lambda_{k}\}(\sum^k_{i=1}n_{i}=N)$，
$\lambda_{i}$对应一个$n_{i}$维的解空间，该空间所有的向量$X$都满足$AX=\lambda_{i} X$）。
\begin{align}
    U=&\begin{pmatrix}
        X_{1} & X_{2} & \dots & X_{N}
    \end{pmatrix}\\
    \intertext{\hspace*{\fill}(根据式\ref{eq:OrthogonalEigenVector}, 有$U^{\dagger}U=I$, 即$U^{\dagger}=U^{-1}$)\hspace*{\fill}}\\
    U^{\dagger}AU=&
    \begin{pmatrix}
        X_{1}^{\dagger} \\ X_{2}^{\dagger} \\ \vdots \\ X_{N}^{\dagger}
    \end{pmatrix}
    A
    \begin{pmatrix}
        X_{1} & X_{2} & \dots & X_{N}
    \end{pmatrix}\\
    =&
    \begin{pmatrix}
        X_{1}^{\dagger} \\ X_{2}^{\dagger} \\ \vdots \\ X_{N}^{\dagger}
    \end{pmatrix}
    \begin{pmatrix}
        \lambda_{1}X_{1} & \lambda_{2}X_{2} & \dots & \lambda_{N}X_{N}
    \end{pmatrix}\\
    =&\begin{pmatrix}
        \lambda_{1} &  &  & \\
        & \lambda_{2} &  & \\
        &  & \ddots & \\
        &  &  & \lambda_{N}
    \end{pmatrix}
\end{align}

\subsection{回到多维高斯函数}
\begin{gather}
    U^{-1}\Sigma'U = \Sigma = 
    \begin{pmatrix}
        \sigma_{1}^2 &  &  & \\
        & \sigma_{2}^2 &  & \\
        &  & \ddots & \\
        &  &  & \sigma_{n}^2
    \end{pmatrix}\\
    \Sigma^{-1}=U^{-1}\Sigma'^{-1}U\\
    X^{\dagger}\Sigma^{-1}X = X^{\dagger}U^{-1}\Sigma'^{-1}UX = X^{\dagger}U^{\dagger}\Sigma'^{-1}UX = (UX)^{\dagger}\Sigma'^{-1}UX\\
    X' = UX\\
    X^{\dagger}\Sigma^{-1}X = X'^{\dagger}\Sigma'^{-1}X'\\
    \intertext{\hspace*{\fill}(实数情况下，$X^{\dagger} = X^{T}$, $U^{\dagger} = U^{T}$, $\Sigma^{\dagger}=\Sigma \rightarrow \Sigma^{T}=\Sigma$)\hspace*{\fill}}
\end{gather}
\begin{gather}
    det(\Sigma') = det(U\Sigma U^{-1}) = det(U)\cdot det(\Sigma)\cdot det(U^{-1}) = det(\Sigma)
\end{gather}
忽略均值矩阵$M$，式\ref{eq:multiDimGassian2}等于：
\begin{align}
    \frac{1}{(2\pi)^{\frac{n}{2}}(det(\Sigma'))^{\frac{1}{2}}}e^{-\frac{1}{2}X'^T\Sigma'^{-1}X'}
\end{align}

\subsection{再议协方差矩阵}

\begin{gather}
    X^{k}=
    \begin{pmatrix}
        x_{1}^{k}\\
        x_{2}^{k}\\
        \vdots \\
        x_{n}^{k}
    \end{pmatrix}
\end{gather}

\begin{gather}
    M=\begin{pmatrix}
        \mu_{1}\\
        \mu_{2}\\
        \vdots \\
        \mu_{n}
    \end{pmatrix}
    =\frac{1}{m}\sum_{k}^{m}
    \begin{pmatrix}
        x_{1}^{k}\\
        x_{2}^{k}\\
        \vdots \\
        x_{n}^{k}
    \end{pmatrix}
    =\frac{1}{m}\sum_{k}^{m} X^{k}\\
    \rightarrow M = \int p(x_{1},x_{2}...x_{n})
    \begin{pmatrix}
        x_{1}\\
        x_{2}\\
        \vdots \\
        x_{n}
    \end{pmatrix}
    dx_{1}dx_{2}...dx_{n}
    = \int p(X) X dX
\end{gather}

\begin{gather}
    \Sigma = \frac{1}{m} \sum_{k}^{m}
    \begin{pmatrix}
        x_{1}^{k}-\mu_{1}\\
        x_{2}^{k}-\mu_{2}\\
        \vdots \\
        x_{n}^{k}-\mu_{n}
    \end{pmatrix}
    \begin{pmatrix}
        x_{1}^{k}-\mu_{1}&x_{2}^{k}-\mu_{2}&...&x_{n}^{k}-\mu_{n}
    \end{pmatrix}
    =\frac{1}{m} \sum_{k}^{m} (X^{k}-M)(X^{k}-M)^{T}\\
    \rightarrow \Sigma = \int p(x_{1},x_{2}...x_{n})
    \begin{pmatrix}
        x_{1}-\mu_{1}\\
        x_{2}-\mu_{2}\\
        \vdots \\
        x_{n}-\mu_{n}
    \end{pmatrix}
    \begin{pmatrix}
        x_{1}-\mu_{1}&x_{2}-\mu_{2}&...&x_{n}-\mu_{n}
    \end{pmatrix}
    dx_{1}dx_{2}...dx_{n}\\
    = \int p(X) (X-M)(X-M)^{T} dX
\end{gather}

\begin{gather}
    X\rightarrow X'=UX\\
    M\rightarrow M'=UM\\
\end{gather}
\begin{align}
    \Sigma \rightarrow \Sigma'
    =& \frac{1}{m} \sum_{k}^{m} (X'^{k}-M')(X'^{k}-M')^{T}\\
    =& \frac{1}{m} \sum_{k}^{m} (UX^{k}-UM)(UX^{k}-UM)^{T}\\
    =& \frac{1}{m} \sum_{k}^{m} U(X^{k}-M)(X^{k}-M)^{T}U^{T}\\
    =& U \Sigma U^{T}
\end{align}

在
\begin{gather}
    X\rightarrow X'=f(X) =
    \begin{pmatrix}
        x'_{1}(x_{1},x_{2}...x_{n})\\
        x'_{2}(x_{1},x_{2}...x_{n})\\
        \vdots\\
        x'_{2}(x_{1},x_{2}...x_{n})
    \end{pmatrix}
\end{gather}
变换下，概率密度$p(X)dX = p'(X')dX'$保持不变
\begin{gather}
    \frac{p'(X')}{p(X)}=\frac{dX}{dX'}
\end{gather}
上式右边代表的是微元$dX$和微元$dX'$的体积比，实际可用雅可比行列式计算：
\begin{gather}
    \left | \left |
    \begin{matrix}
        \frac{dX}{dX'}
    \end{matrix}
    \right | \right |
    = \left | \left |
    \begin{matrix}
        \frac{\partial x_{1}}{\partial x'_{1}} & \frac{\partial x_{1}}{\partial x'_{2}} & ... & \frac{\partial x_{1}}{\partial x'_{n}}\\
        \frac{\partial x_{2}}{\partial x'_{1}} & \frac{\partial x_{2}}{\partial x'_{2}} & ... & \frac{\partial x_{2}}{\partial x'_{n}}\\
        \vdots & \vdots & \ddots & \vdots\\
        \frac{\partial x_{n}}{\partial x'_{1}} & \frac{\partial x_{n}}{\partial x'_{2}} & ... & \frac{\partial x_{n}}{\partial x'_{n}}
    \end{matrix}
    \right | \right |
\end{gather}

\begin{gather}
    p'(X') = p(X)
    \left | \left |
    \begin{matrix}
        \frac{dX}{dX'}
    \end{matrix}
    \right | \right |
    = p(f^{-1}(X'))
    \left | \left |
    \begin{matrix}
        \frac{dX}{dX'}
    \end{matrix}
    \right | \right |
\end{gather}

\subsection{转动惯量平行轴定理}

%\begin{gather}
%    m=\int \rho(x,y)dxdy\\
%    \mu_{x}=\int \rho(x,y)xdxdy\\
%    \mu_{y}=\int \rho(x,y)ydxdy\\
%    I_{\mu} = \int \rho(x,y)((x-\mu_{x})^{2}+(y-\mu_{y})^{2})dxdy\\
%    \int \rho(x,y)((x-x_{a})^{2}+(y-y_{a})^{2})dxdy\\
%    =\int \rho(x,y)(((x-\mu_{x})-(x_{a}-\mu_{x}))^{2}+((y-\mu_{y})-(y_{a}-\mu_{y}))^{2})dxdy\\
%    =\int \rho(x,y)((x-\mu_{x})^{2}+(y-\mu_{y})^{2}\\
%    +(x_{a}-\mu_{x})^{2}+(y_{a}-\mu_{y})^{2}\\
%    -2(x-\mu_{x})(x_{a}-\mu_{x})-2(y-\mu_{y})(y_{a}-\mu_{y}))dxdy\\
%    =I_{\mu}+m((x_{a}-\mu_{x})^{2}+(y_{a}-\mu_{y})^{2})+0
%\end{gather}

\begin{gather}
    \overrightarrow{r}=
    \begin{pmatrix}
        x_{1}\\
        x_{2}\\
        \vdots\\
        x_{n}
    \end{pmatrix}
    \quad
    d\bm{r} = dx_{1}dx_{2}...dx_{n}\\
    m=\int \rho(\overrightarrow{r})d\bm{r}\\
    \overrightarrow{\mu}=\int \rho(\overrightarrow{r})\overrightarrow{r}d\bm{r}\\
    I_{\mu} = \int \rho(\overrightarrow{r})(\overrightarrow{r}-\overrightarrow{\mu})^{2}d\bm{r}\\
    \int \rho(\overrightarrow{r})(\overrightarrow{r}-\overrightarrow{r_{A}})^{2}d\bm{r}\\
    =\int \rho(\overrightarrow{r})((\overrightarrow{r}-\overrightarrow{\mu})-(\overrightarrow{r_{A}}-\overrightarrow{\mu}))^{2}d\bm{r}\\
    =\int \rho(\overrightarrow{r})((\overrightarrow{r}-\overrightarrow{\mu})^{2}
    +(\overrightarrow{r_{A}}-\overrightarrow{\mu})^{2}
    -2(\overrightarrow{r}-\overrightarrow{\mu})\cdot(\overrightarrow{r_{A}}-\overrightarrow{\mu}))d\bm{r}\\
    =I_{\mu}+m(\overrightarrow{r_{A}}-\overrightarrow{\mu})^{2}+0
\end{gather}

\subsubsection{分辨率与卷积}

假定一个观测量$x$的自然分布为$p_{nature}(x)$，对任何观测量的测量都是不可能完全精确的，用分辨率来描述测量的精确度，
考虑分辨率后观测量$x$的测量分布为：
\begin{gather}
    \label{eq:reso1}
    p_{measure}(x)=\int p_{nature}(x')R(x-x',\sigma_{R}(x'))dx'
\end{gather}
$R(x-x',\sigma_{R}(x'))$表示当观测量的真实值为$x'$的时候，测量值$x$的分布，
$\sigma_{R}(x')$是$R$分布的标准差，
对于一个好的测量，其$R$分布的中心值为真实值$x'$。
如果假定$R$分布的标准差与观测量的真实值$x'$无关，则式\ref{eq:reso1}变为$p_{nature}(x')$和$R(x-x')$的卷积：
\begin{gather}
    p_{measure}(x)=\int p_{nature}(x')R(x-x',\sigma_{R})dx'
\end{gather}

\begin{gather}
    \mu=\int p_{nature}(x)xdx\\
    \sigma^{2} = \int p_{nature}(x)(x-\mu)^{2}dx
\end{gather}
\begin{align}
    &\int p_{measure}(x)xdx\\
    =&\int \int p_{nature}(x')R(x-x',\sigma_{R}) x dx'dx\\
    =&\int (\int R(x-x',\sigma_{R}) xdx)p_{nature}(x') dx'\\
    =&\int x'p_{nature}(x') dx'\\
    =&\mu
\end{align}
\begin{align}
    &\int p_{measure}(x)(x-\mu)^{2}dx\\
    =&\int p_{measure}(x)((x-x')-(\mu-x'))^{2}dx\\
    =&\int (\int p_{nature}(x')R(x-x',\sigma_{R})dx')((x-x')^{2}+(\mu-x')^{2}-2(x-x')(\mu-x'))dx\\
    =&\int p_{nature}(x') (\int R(x-x',\sigma_{R})((x-x')^{2}+(\mu-x')^{2}-2(x-x')(\mu-x'))dx)dx'\\
    =&\int p_{nature}(x') (\sigma_{R}^{2}+(\mu-x')^{2}-0)dx'\\
    =&\sigma_{R}^{2}+\sigma^{2}
    \label{eq:measuredSigma}
\end{align}
式\ref{eq:measuredSigma}表明，如果$\sigma_{R}$不依赖于观测量真实值$x'$，
则$x$的测量分布的方差为其自然分布方差与分辨率$\sigma_{R}$的平方的和。

事实上，类比转动惯量的平行轴定理：
\begin{gather}
    p(x')dx'\rightarrow m\\
    R(x-x',\sigma_{R})\rightarrow \rho(\overrightarrow{r})\\
    \sigma_{R}^{2}\rightarrow I_{\mu}\\
    p(x')dx'(x'-\mu)^{2}+\sigma_{R}^{2}\rightarrow m(\overrightarrow{\mu} -\overrightarrow{r_{A}})^{2}+I_{\mu}\label{eq:analogy1}
\end{gather}
式\ref{eq:analogy1}左边对$x'$积分：
\begin{gather}
    \int p(x')dx'(x'-\mu)^{2} + \int p(x')dx'\sigma_{R}^{2}=\sigma^{2}+\sigma_{R}^{2}
\end{gather}

\section{卡尔曼滤波}

简单地概括一下卡尔曼滤波的思想，
从一个初始状态出发，初始状态包括初始状态及其误差，
利用（理论）模型将一个时刻的状态作为输入，预测下一个时刻的状态，误差通过模型也可以传递到下一个时刻，
模型可拆解为三部分：无外部干涉下系统的自演化、外部干涉对系统的改变、未纳入建模的其它环境扰动因素，
假定可建模部分的模型能完美描述纳入模型考虑的实际情况，则预测的不确定性仅来自未纳入建模的扰动因素
\footnote{
    预测分为：可建模部分、缺乏足够认识或建模复杂度比较高而难建模的部分，
    将可建模部分拆解为两部分：无外部干涉下系统的自演化、外部干涉对系统的改变，
    预测的误差包括：建模部分的误差、缺少建模导致的不确定性，
    建模部分的误差包括：理论不能完美描述实际情况、模型的计算精度
}，
预测本身含有的不确定性和作为输入的上一状态的误差共同构成预测结果的误差，
对某一个时刻的预测值及其误差构成该时刻的预测分布，
对某个时刻的状态进行测量，测量值及测量仪器的精度范围构成该时刻的测量分布，
综合两个分布的“意见”，取一个折衷方案，将两个分布相乘得到新的分布，新分布的最概然值和标准差确定为当前状态值及其误差。
